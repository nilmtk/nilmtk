{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's tinker with some ideas for how the pipe will fit into the broader NILMTK framework.\n",
      "My hunch is that we should think of NILMTK in terms of layers of abstraction.  From the bottom up:\n",
      "\n",
      "--------------\n",
      "\n",
      "### Layer 1 (bottom layer): The physical layer\n",
      "\n",
      "* Deals with: retrieving data from disk / network / direct from a meter\n",
      "* Optimised for: handling large amounts of data\n",
      "* Services it provides: delivering a pd.DataFrame of data given a specific mask and columns\n",
      "* Totally agnostic about what the data 'means'.  It could be voltage, current, temperature, PIR readings etc.\n",
      "\n",
      "#### Classes in the Physical layer\n",
      "\n",
      "##### DataStore \n",
      "\n",
      "* loads a single chunk at a time from physical location and returns a DataFrame\n",
      "* subclasses for NILMTK HDF5, NILMTK CSV, Xively, REDD, iAWE, UKPD, etc; MetOffice XLS data, Current Cost meters etc.\n",
      "* One DataStore per HDF5 file or folder or CSV files or Xively feed etc\n",
      "* always use JSON for metadata\n",
      "* methods:\n",
      "  * __init__(start_date, end_date): start_date and end_date allow us to set a \"window of interest\", i.e. to crop the data\n",
      "    non-destructively.\n",
      "  * load(key, start_date, end_date): returns DataFrame, raises MemoryError if we try to load too much data.  key is the\n",
      "    location of a table.  There is assumed to be a one-to-one mapping from tables to meters. The hierarchical structure \n",
      "    of key is\n",
      "    standardised across NILMTK e.g. building1/utility/electric/meter4.  Or, if the physical store only represents,\n",
      "    say, a single house then the path is truncated to utility/electric/meter4.\n",
      "  * load_metadata(key): load metadata for a given key location\n",
      "  * write(key, dataframe) # if key exists then data will be appended.\n",
      "  * write_metadata(key, dict) # always overwrites\n",
      "  * get_generator(key, periods): periods is a list of (start_date, end_date) tuples.\n",
      "  * building_numeric_ids(): get a list of available building IDs\n",
      "  * meter_numeric_ids(building_numeric_id): get a list of available meter IDs\n",
      "  * appliance_ids(building_numeric_id): get a list of available appliances\n",
      "* in the future, this could be responsible for efficiently compacting and uncompacting data (e.g. finding tables with common indicies and putting them into the same table perhaps score similarity by doing `len(index1 & index2) / max(len(index1), len(index2))`; finding data which only has integer values and converting from float32 to int32 or even uint16)\n",
      "\n",
      "##### Loader\n",
      "\n",
      "* owns exactly one DataStore\n",
      "* knows how to chunk up data using masks\n",
      "* returns a generator\n",
      "* attributes\n",
      "  * store : DataStore\n",
      "  * key : string (the key into the DataStore)\n",
      "  * mask : Mask (basically a list of periods, i.e. a list of (start_date, end_date) tuples.  Used for masking out gaps; or selecting only contiguous sections)\n",
      "  * period : string; a Pandas period alias e.g. 'D', 'M' etc\n",
      "  * new_chunk_at_mask_boundaries : bool; default True\n",
      "  * persist : bool; defaults to false; whether to keep last chunk in memory\n",
      "  * start_date : datetime\n",
      "  * end_date : datetime\n",
      "* methods\n",
      "  * load(measurements_to_load) : returns generator\n",
      "  * load_metadata()\n",
      "  * reset() -> reset mask and period\n",
      "  * unload(); only useful if persist==True\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "### Layer 2: The meter layer\n",
      "\n",
      "#### Classes in the meter layer\n",
      "\n",
      "##### Meter\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We're trying to do \"out of core\" or \"external memory\" computations.\n",
      "* I think [PyTables.Expr class](http://pytables.github.io/usersguide/libref/expr_class.html) ([more info](http://www.pytables.org/moin/ComputingKernel)) (which is similar to numexpr) can help with loading large files off disk and doing a small set of functions (e.g. exponentiation, addition, subtraction etc), but *we can't pass in our own functions*.\n",
      "* So perhaps our best strategy is to load reasonably large chunks (up to, say, 1/8 of system memory) from disk.  Then each node in the pipeline could use numexpr to automatically further reduce these chunks to a size that can fit into CPU cache, but only for primiative ops.  Although, it's also the case that once the data is in the CPU cache, we should do as much in the CPU cache as possible, so maybe we should use smallish (1MB?  64KB???) chunks so that the data only has to be loaded into cache once?  See the 2010 PDF on \"[why modern CPUs are starving and what can be done about it](http://www.blosc.org/docs/StarvingCPUs-CISE-2010.pdf)\".  but but but... this is all getting rather complicated.  For the first version, I think we should stick to the minimal viable product (i.e. load, say, data in chunk sizes of 512MB and only allow each node to see the data once).\n",
      "* The is some evidence (e.g. see [this SO discussion](http://stackoverflow.com/questions/20940805/python-particles-simulator-out-of-core-processing), especially the last answer and associated iPyNB) that it may be faster to use PyTables directly rather than via Pandas (this would also allow us to use uint32 for timestamps, where appropriate); although I think we should go ahead and use Pandas, at least for now.\n",
      "* [\"Large data work flows using pandas\"](http://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas/) is also of interest, especially Jeff's answer.  I need to get my head around the implications of pytables storing row-wise???\n",
      "* [here's a discussion (and code) on pytables google groups](https://groups.google.com/forum/#!topic/pytables-users/pCgp-vyP4Zk) about doing out-of-core matrix x matrix using pytables (not using tables.Expr or numexpr)\n",
      "* numpy has [memmap](http://docs.scipy.org/doc/numpy/reference/generated/numpy.memmap.html) for doing OOC, but it seems that PyTables is better.\n",
      "* [Here's a 2013 paper](http://www.ncbi.nlm.nih.gov/pubmed/23479353) for doing OOC genome studies using Java and HDF5.\n",
      "* I think my i5-450M has 64KB L1, 2x256KB L2 and shared 3MB L3.  \n",
      "* blosc seems like a very high performance compression algo\n",
      "* I should like into the [\"biggus\" Python project](https://github.com/SciTools/biggus) (for doing OOC) (I'm not sure but I think this is mostly useful for doing stuff where the results of a calc are too big to fit into mem; I'm not sure it's useful for loading stuff from disk chunk by chunk) (all the code appears to be in `biggus/__init__.py`)\n",
      "\n",
      "\n",
      "Some issues which require some more thought:\n",
      "\n",
      "#### Do any nodes require multiple passes over the data?\n",
      "\n",
      "E.g. calculation of variance?  Could use [on-line or parallel algo listed on wikipedia](http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm).\n",
      "\n",
      "Or, if we really need multiple passes, then each Node could specify an \"n_passes\" precondition (?) and Pipeline will then run the pipeline up to an including that node n times (after which it will pass data onto the rest of the pipeline?)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Pipeline(object):\n",
      "    \"\"\"\n",
      "    A data processing pipeline for processing power data.  Operates at\n",
      "    the \"Meter\" layer.  The basic motivation is that we want to be \n",
      "    able to do a sequence of processing steps on a chunk\n",
      "    while that chunk is in memory.\n",
      "    \n",
      "    SOURCE -> LOADER/SPLITTER -> NODE_1 -> ... -> NODE_N\n",
      "    \n",
      "    A pipeline consists of one loader/splitter\n",
      "    node which loads and, if necessary, splits the data into chunks;\n",
      "    if there are K chunks then the pipeline runs K times; and on each\n",
      "    iteration the output from the loader/splitter is a single DataFrame\n",
      "    (with metatdata).\n",
      "    \n",
      "    The Loader contains a Source object which defines how to pull\n",
      "    data from the physical data store (disk / network / device).\n",
      "    \n",
      "    After the loader/splitter are an arbitrary number of \"nodes\"\n",
      "    which process data in sequence or export the data to disk.\n",
      "    \n",
      "    Each processing node has a set of preconditions (e.g. gaps must be\n",
      "    filled) and a set of postconditions (e.g. gaps will have been\n",
      "    filled).  This allows us to check that a particular pipeline is\n",
      "    viable (i.e. that, for every node, the node's preconditions are\n",
      "    satisfied by an upstream node or by the source).\n",
      "    \n",
      "    During a single cycle of the pipeline, results from each\n",
      "    stats node are stored in the `dataframe.results` dict.  At the end\n",
      "    of each pipeline cycle, the contents of dataframe.results \n",
      "    are combined and the aggregate results are stored in the pipeline.\n",
      "    \n",
      "    IDEAS FOR THE FUTURE???:\n",
      "    Pipelines could be saved/loaded from disk.\n",
      "    \n",
      "    If the pipeline was represented by a directed acyclic\n",
      "    graphical model (DAG) then:\n",
      "      pipeline could fork into multiple parallel\n",
      "      pipelines.  Data and metadata would be copied to each fork and\n",
      "      each sub-pipeline would be run as a separate process (after\n",
      "      checking requirements for each subpipeline as the start).\n",
      "    \n",
      "      Pipelines could be rendered\n",
      "      graphically.  In the future it would be nice to have a full\n",
      "      graphical UI (like Node-RED).\n",
      "    \n",
      "    Attributes\n",
      "    ----------\n",
      "    nodes : list of Node objects\n",
      "    loader : Loader\n",
      "    results : dict of Results objects storing aggregate stats results\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    >>> table_path = 'building1/utility/electric/meter1'\n",
      "    >>> source = HDFTableSource('ukpd.h5', table_path)\n",
      "    >>> loader = Loader(source, start=\"2013-01-01\", end=\"2013-06-01\")\n",
      "\n",
      "    Calculate total energy and save the preprocessed data\n",
      "    and the energy data back to disk:\n",
      "    \n",
      "    >>> nodes = [BookendGapsWithZeros(), \n",
      "                 Energy(), \n",
      "                 HDFTableExport('meter1_preprocessed.h5', table_path)]\n",
      "    >>> pipeline = Pipeline(loader, nodes).run()\n",
      "    >>> energy = pipeline.results['energy']\n",
      "    >>> print(\"Energy in Joules =\", energy.joules, \"and kWh =\", energy.kwh)\n",
      "    \n",
      "    \"\"\"\n",
      "    def __init__(self, loader=None, nodes=None):\n",
      "        self.loader = loader\n",
      "        self.nodes = nodes\n",
      "        self.results = {}\n",
      "        \n",
      "    def strings_to_nodes(self, strings):\n",
      "        \"\"\"Converts string representation of nodes to \n",
      "        actual node objects\"\"\"\n",
      "        \n",
      "    def nodes_to_strings(self):\n",
      "        \"\"\"Returns a string representation of each node,\n",
      "        as a list of strings.\"\"\"\n",
      "    \n",
      "    def run(self):\n",
      "        self.reset()\n",
      "        self.check_preconditions()\n",
      "        # Run pipeline\n",
      "        for chunk in self._loader.load_chunks(conditions.measurements()):\n",
      "            processed_chunk = self._run_chunk_through_pipeline(chunk)\n",
      "            self._update_results(processed_chunk.results)\n",
      "\n",
      "    def _run_chunk_through_pipeline(self, chunk):\n",
      "        for node in nodes:\n",
      "            chunk = node.process(chunk)\n",
      "        return chunk\n",
      "    \n",
      "    def _update_results(self, results_for_chunk):\n",
      "        for statistic, result in results_for_chunk.iteritems():\n",
      "            try:\n",
      "                self.results[statistic].update(result)\n",
      "            except KeyError:\n",
      "                self.results[statistic] = result\n",
      "                    \n",
      "    def reset(self):\n",
      "        self.results = {}\n",
      "        for node in self.nodes:\n",
      "            node.reset()\n",
      "            \n",
      "    def check_preconditions(self):\n",
      "        assert(isinstance(self.source, Source))\n",
      "        assert(isinstance(self.nodes, list))\n",
      "        assert(len(self.nodes) > 0))\n",
      "        \n",
      "        # Check requirements\n",
      "        condition = self._loader.metadata\n",
      "\n",
      "        # for example, `condition` might represent:\n",
      "        # * [measurements] are available\n",
      "        # * gaps = Gaps([(\"2013-01-01 00:00\", \"2013-01-01 00:10\"), ...])\n",
      "        # * zeros have been inserted\n",
      "        # * sample_period = 1 second (I was thinking of putting this\n",
      "        #   in a separate, imutable 'metadata' field but sample_period\n",
      "        #   could be changed during the pipeline by an\n",
      "        #   up/down-sampling node)\n",
      "\n",
      "        for node in self.nodes: # go through the nodes in order, starting upstream\n",
      "            node.check_preconditions(condition)\n",
      "            condition.update(node.postconditions)\n",
      "            # for example, `condition` might now include:\n",
      "            # * total energy for this chunk has been calculated\n",
      "            # * from [measurements] available, we must load reactive power and active power\n",
      "\n",
      "\n",
      "        \n",
      "\n",
      "class Node(object):\n",
      "    \"\"\"Abstract class defining interface for all Node subclasses,\n",
      "    where a 'node' is a module which runs pre-processing or statistics\n",
      "    or NILM training or disaggregation.\n",
      "    \"\"\"\n",
      "\n",
      "    postconditions =  {} # TODO\n",
      "\n",
      "    def __init__(self, name):\n",
      "        self.name = name\n",
      "\n",
      "    @staticmethod\n",
      "    def check_preconditions(conditions):\n",
      "        \"\"\"\n",
      "        Parameters\n",
      "        ----------\n",
      "        conditions : dict\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        boolean\n",
      "        \n",
      "        Description\n",
      "        -----------\n",
      "        \n",
      "        Requirements can be of the form:\n",
      "    \n",
      "        \"node X needs (power.apparent or power.active) (but not\n",
      "        power.reactive) and voltage is useful but not essential\"\n",
      "    \n",
      "        or\n",
      "    \n",
      "        \"node Y needs everything available from disk (to save to a copy to\n",
      "        disk)\"\n",
      "    \n",
      "        or\n",
      "    \n",
      "        \"ComputeEnergy node needs gaps to be bookended with zeros\" (if\n",
      "        none of the previous nodes provide this service then check\n",
      "        source.metadata to see if zeros have already been inserted; if the\n",
      "        haven't then raise an error to tell the user to add a\n",
      "        BookendGapsWithZeros node.)\n",
      "        \"\"\"\n",
      "        # TODO: see if there are any unsatisfied preconditions\n",
      "        # if there are then raise an UnsatisfiedPreconditionsError\n",
      "        # giving the exact precondition that failed, why it failed\n",
      "        # which node is complaining, and suggestions for how to fix it\n",
      "        pass\n",
      "        \n",
      "    def process(self, df):\n",
      "        # check_preconditions again??? (in case this node is not run in\n",
      "        # the context of a Pipeline?)\n",
      "        # do stuff to df\n",
      "        return df\n",
      "    \n",
      "#-------------- RESULT CLASSES -----------------#\n",
      "\n",
      "class Results(pd.DataFrame):\n",
      "    \"\"\"Metadata results from each node need to be assigned to a specific\n",
      "    class so we know how to combine results from multiple chunks.  For\n",
      "    example, Energy can be simply summed; while dropout rate should be\n",
      "    averaged, and gaps need to be merged across chunk boundaries.  Results\n",
      "    objects contain a DataFrame, the index of which is the start timestamp for\n",
      "    which the results are valid; the first column ('end') is the end\n",
      "    timestamp for which the results are valid.  Other columns are accumaltors for the results.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    _accumulator : DataFrame\n",
      "        Index is period start.  Columns are: end_date and any columns for internal storage of stats.\n",
      "    \"\"\"\n",
      "    \n",
      "    @property\n",
      "    def combined(self):\n",
      "        \"\"\"Return all results from each chunk combined.\n",
      "        Either return single float for all periods or a dict where necessary, \n",
      "        e.g. if calculating Energy for a meter which records both apparent power and\n",
      "        active power then we've have energyresults.combined['active']\n",
      "        \"\"\"\n",
      "        pass\n",
      "        \n",
      "    @property\n",
      "    def per_period(self):\n",
      "        \"\"\"return a DataFrame.  Index is period start.  Columns are: end_date and <stat name>\n",
      "        \"\"\"\n",
      "    \n",
      "    def update(self, new_result):\n",
      "        \"\"\"Update with new results\"\"\"\n",
      "        pass\n",
      "\n",
      "\n",
      "class EnergyResults(Results):\n",
      "    @property\n",
      "    def combined(self):\n",
      "        return self.sum()\n",
      "\n",
      "#-------------- STATS NODES --------------------#\n",
      "    \n",
      "class StatsNode(Object):\n",
      "    \"\"\"\n",
      "    Abstract class for nodes which process statistics\n",
      "    \"\"\"\n",
      "    def results(self):\n",
      "        pass\n",
      "\n",
      "\n",
      "class LocateGapsNode(Node, StatsNode):\n",
      "    \n",
      "    postconditions = {} # Gaps will be identified!\n",
      "    \n",
      "    @staticmethod\n",
      "    def check_preconditions(conditions):\n",
      "        # Needs sample period to be set\n",
      "        # must be followed by a buffer node or an export node\n",
      "        \n",
      "class LocateContiguousSections(Node, StatsNode):\n",
      "    pass\n",
      "\n",
      "class EnergyNode(Node, StatsNode):\n",
      "    \"\"\"Computes energy\"\"\"\n",
      "\n",
      "    name = 'energy'\n",
      "    \n",
      "    postconditions = {} # Energy is calculated\n",
      "\n",
      "    @staticmethod\n",
      "    def check_preconditions(conditions):\n",
      "        # Needs any power or energy measurements.  Preference is for\n",
      "        # energy measurements.  Computes energy for each of {active,\n",
      "        # reactive, apparent} available If only power measurements are\n",
      "        # available and if there are gaps then requires zeros to be\n",
      "        # inserted before gaps.\n",
      "           \n",
      "    def process(self, df):\n",
      "        # TODO: calculate energy_for_df\n",
      "        df.results[(self.name, self.instance)] = EnergyResults(energy_for_df)\n",
      "        return df\n",
      "    \n",
      "    def results(self):\n",
      "        return self._cumulator\n",
      "\n",
      "class ProportionEnergySubmeteredNode(Node, StatsNode):\n",
      "    def preconditions(self):\n",
      "        \"\"\"\n",
      "        * use the gaps in mains as a mask\n",
      "        * then calculate energy\n",
      "        \"\"\"\n",
      "\n",
      "#---------------- PROCESSING NODES --------------------#\n",
      "\n",
      "class ProcessingNode(Object):\n",
      "    \"\"\"\n",
      "    Abstract class for nodes which process data\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "class BookendGapsWithZerosNode(Node, ProcessingNode):\n",
      "    \n",
      "    postconditions = {} # Bookends gaps with zeros\n",
      "    \n",
      "    @staticmethod\n",
      "    def check_preconditions(conditions):\n",
      "        \"\"\"Requires gaps to be located.\"\"\"\n",
      "            \n",
      "    def process(self, df):\n",
      "        for gap_start, gap_end in df.metadata.gaps:\n",
      "            # insert zeros!\n",
      "            \n",
      "        return df\n",
      "    \n",
      "#--------------- EXPORT NODES --------------------#\n",
      "\n",
      "class ExportNode(Object):\n",
      "    \"\"\"\n",
      "    Abstract class for nodes which export data to disk / network etc\n",
      "    \"\"\"\n",
      "    pass\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Mask(object):\n",
      "    \"\"\"\n",
      "    Attributes\n",
      "    ----------\n",
      "    _mask : list of (start_datetime, end_datetime) tuples\n",
      "    \"\"\"\n",
      "    \n",
      "    def inverse(self):\n",
      "        \"\"\"Return new Mask inverted\"\"\"\n",
      "\n",
      "\n",
      "class Meter(object):\n",
      "    \"\"\"Represents a physical meter.\n",
      "    \n",
      "    Attributes\n",
      "    ----------\n",
      "    loader : Loader\n",
      "    \n",
      "    metadata : dict.  Including keys:\n",
      "        Required:\n",
      "        id : int, meter ID\n",
      "       \n",
      "        submeter_of : int, ID of upstream meter\n",
      "       \n",
      "        preprocessing : list of strings (why not actual Node objects?), each describing a preprocessing Node.\n",
      "          preprocessing to be applied before returning any stats answers; or before exporting.\n",
      "          e.g. power normalisation or removing gaps\n",
      "\n",
      "        type: reference to a MeterType object:\n",
      "          manufacturer : string\n",
      "          model : string        \n",
      "          sample_period : float, seconds\n",
      "          max_sample_period : float, seconds\n",
      "          measurement_limits : dict.  e.g. {('power', 'active'): [0,3000], ('voltage', ''): [150,250]}\n",
      "    \n",
      "    \"\"\"\n",
      "    def import(self, store, key):\n",
      "        self.metadata = store.load_metadata(key)\n",
      "        self.loader = Loader(store, key)\n",
      "\n",
      "    def export(self, destination, key):\n",
      "        \"\"\"\n",
      "        Convert all relevant attributes to a dict to be \n",
      "        saved as metadata in destination at location specified\n",
      "        by key\n",
      "        \"\"\"\n",
      "        destination.write_metadata(key, self.metadata)\n",
      "                \n",
      "    def power_series(self, measurement_preferences=None, \n",
      "                     required_measurement=None,\n",
      "                     normalise=False, voltage_series=None, \n",
      "                     nominal_voltage=None):\n",
      "        \"\"\"Power timeseries.\n",
      "        \n",
      "        Set meter.loader parameters to configure chunk sizes, start date etc.\n",
      "        \n",
      "        The following cleaning steps will be run if the relevant entries\n",
      "        in meter.cleaning are True:\n",
      "\n",
      "        * remove implausable values\n",
      "        * gaps will be bookended with zeros\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        measurement_preferences : list of Measurements, optional. Defaults to active > apparent > reactive\n",
      "        required_measurements : Measurement, optional.  Raises MeasurementError if not available.\n",
      "        normalise : boolean, optional, defaults to False\n",
      "        voltage_series : Meter object with voltage measurements available.  If not supplied and if normalise is True\n",
      "            then will attempt to use voltage data from this meter.\n",
      "        nominal_voltage : float\n",
      "\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        generator of pd.Series of power measurements.\n",
      "        \"\"\"\n",
      "        \n",
      "    def voltage_series(self):\n",
      "        \"\"\"Returns a generator of pd.Series of voltage, if avilable.\"\"\"\n",
      "        \n",
      "    def total_energy(self, preprocessing=None):\n",
      "        \"\"\"returns an EnergyResults object\"\"\"\n",
      "        nodes = [] if preprocessing is None else preprocessing\n",
      "        nodes.extend([RemoveImplausableValues(self.measurement_limits), \n",
      "                      BookendGapsWithZeros(), \n",
      "                      Energy()])\n",
      "        pipeline = Pipeline(self, nodes)\n",
      "        pipeline.run()\n",
      "        return pipline.results['energy']\n",
      "        \n",
      "    def dropout_rate(self):\n",
      "        \"\"\"returns a DropoutRateResults object.\"\"\"\n",
      "        \n",
      "    def gaps(self): \n",
      "        \"\"\"returns Mask object\"\"\"\n",
      "        \n",
      "    def contiguous_sections(self):\n",
      "        \"\"\"retuns Mask object\"\"\"\n",
      "        \n",
      "    def clean_and_export(self, destination_datastore):\n",
      "        \"\"\"Apply all cleaning configured in meter.cleaning and then export.  Also identifies\n",
      "        and records the locations of gaps.  Also records metadata about exactly which\n",
      "        cleaning steps have been executed and some summary results (e.g. the number of\n",
      "        implausible values removed)\"\"\"\n",
      "        \n",
      "    def set_loader_attributes(self, **kwargs):\n",
      "        \"\"\"Provides a common interface to setting loader attributes.\n",
      "        e.g. set_load_attributes(mask=Mask())\n",
      "        \"\"\"\n",
      "        for key, value in kwargs.iteritems():\n",
      "            self._loader.__setattr__(key, value)\n",
      "        \n",
      "    def reset_loader_attributes(self):\n",
      "        self._loader.reset()\n",
      "        \n",
      "    def get_loader_attribute(self, attribute):\n",
      "        return self._loader.__getattr__(attribute)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# EXAMPLES:\n",
      "\n",
      "meter = Meter()\n",
      "store = HDFDataStore('redd.h5', start_date='2011-01-01', end_date='2011-02-01')\n",
      "meter.import(store, 'building1/utility/electric/meter1')\n",
      "\n",
      "# To find total energy in kWh, combined across all chunks:\n",
      "meter.energy().combined\n",
      "\n",
      "# To find energy, after first masking out periods missing from the mains data:\n",
      "meter.set_loader_attributes(mask = mains_meter.gaps())\n",
      "meter.energy().combined\n",
      "\n",
      "# To find dropout rate, excluding gaps:\n",
      "meter.set_loader_attributes(mask = meter.gaps())\n",
      "meter.dropout_rate().combined\n",
      "\n",
      "# To find dropout rate and energy per hour:\n",
      "meter.reset_loader_attributes()\n",
      "meter.set_loader_attributes(period = 'H')\n",
      "energy_per_period = meter.energy().per_period\n",
      "dropout_rate_per_period = meter.dropout_rate().per_period\n",
      "\n",
      "# Plot dropout rate per hour:\n",
      "meter.set_loader_attributes(period = 'H')\n",
      "meter.dropout_rate().per_period.plot()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "##### Pipeline\n",
      "Processes data from a single meter at a time.  Required so that we can run multiple chunks through the pipeline if the meter data is too large to fit into memory.  See above for details.\n",
      "\n",
      "##### Node (and all subclasses)\n",
      "See above for details\n",
      "\n",
      "##### Results (and all subclasses)\n",
      "See above for details\n",
      "  \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-----------\n",
      "\n",
      "### Layer 3: The mains and appliances layer\n",
      "\n",
      "There's a many-to-many mapping between appliance and meters:\n",
      "\n",
      "* sometimes one meter might measure multiple appliances\n",
      "* sometimes one appliance (e.g. Dual Supply) has multiple meters\n",
      "* but many times there is a one-to-one mapping between appliance and meter\n",
      "\n",
      "Also, there's a one-to-many mapping between 'mains feeds' and meters, \n",
      "either because there are multiple splits or phases; \n",
      "or because multiple meters measure the same mains split.\n",
      "\n",
      "General metadata for meters and appliances could be stored in a JSON file within NILMTK and merged with specific info per meter or appliance.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Mains(object):\n",
      "    \"\"\"\n",
      "    Attributes\n",
      "    ----------\n",
      "    meters : list of Meter objects\n",
      "    \"\"\"\n",
      "    def __init__(self, meters):\n",
      "        self.meters = meters\n",
      "        \n",
      "    def export(destination, key):\n",
      "        # Exports metadata and \n",
      "        # list of meters as a list of meter ID ints\n",
      "        \n",
      "    def import(store, key, meters_dict):\n",
      "        # Import metadata\n",
      "        # self.meters = list of relevant meter\n",
      "        # objects taken from meters_dict\n",
      "    \n",
      "    def set_loader_attributes(self, **kwargs):\n",
      "        # TODO: I guess each meter's loader needs to be setup so\n",
      "        # we get chunks spanning the same time windows so, for example,\n",
      "        # we can still calculate energy per time period?\n",
      "        for meter in self.meters:\n",
      "            meter.set_loader_attributes(**kwargs)\n",
      "            \n",
      "    def power_series(self, **kwargs):\n",
      "        \"\"\"Power series.  Sums together three phases / dual split power.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        generator of pd.Series of power measurements.\n",
      "        \"\"\"\n",
      "        # TODO: warn if any meter also measures an appliance\n",
      "        # which isn't this appliance / mains.  User proper\n",
      "        # Python warning: http://docs.python.org/2/library/warnings.html\n",
      "        \n",
      "        if len(self.meters) == 1:\n",
      "            return self.meters[0].power_series(**kwargs)\n",
      "        \n",
      "        # TODO: really not confident the code below is correct!!\n",
      "        power_generators = []\n",
      "        for meter in self.meters:\n",
      "            power_generators.append(meter.power_series(**kwargs))\n",
      "        for generators in zip(power_generators):\n",
      "            power_for_chunk = generators[0]\n",
      "            for generator in generators[1:]:\n",
      "                power_for_chunk += generator\n",
      "            yield power_for_chunk\n",
      "        \n",
      "    def total_energy(self, *args, **kwargs):\n",
      "        \"\"\"Returns EnergyResults object, as if it were a single meter\"\"\"\n",
      "        if len(self.meters) == 1:\n",
      "            return self.meters[0].total_energy(*args, **kwargs)\n",
      "        # TODO.\n",
      "\n",
      "    def gaps(self):\n",
      "        \"\"\"Returns gaps when any meter in self._meters is inactive.\"\"\"\n",
      "    \n",
      "    @property\n",
      "    def preprocessing(self, list_of_nodes):\n",
      "        for meter in self.meters:\n",
      "            meter.preprocessing = list_of_nodes\n",
      "\n",
      "\n",
      "class Appliance(Mains):\n",
      "    \"\"\"\n",
      "    Attributes\n",
      "    ----------\n",
      "    \n",
      "    metadata : pd.DataFrame single row and columns:\n",
      "       name : string\n",
      "       instance : int\n",
      "       \n",
      "       Only need to specify name & instance.  Then NILMTK will get the generic metadata\n",
      "       for that name from the central appliance database.  Any additional\n",
      "       metadata will override defaults.  e.g.:\n",
      "       \n",
      "       on_power_threshold : float, watts\n",
      "       minimum_off_duration : timedelta\n",
      "       minimum_on_duration : timedelta\n",
      "       \n",
      "    mains : Mains (used so appliance methods can default to use\n",
      "      the same measured parameter (active / apparent / reactive) \n",
      "      as Mains; and also for use in proportion of energy submetered\n",
      "      and for voltage normalisation.)\n",
      "       \n",
      "    \"\"\"\n",
      "    def total_on_duration(self):\n",
      "        \"\"\"Return timedelta\"\"\"\n",
      "        pass\n",
      "    \n",
      "    def on_durations(self):\n",
      "        pass\n",
      "    \n",
      "    def activity_distribution(self, bin_size, timespan):\n",
      "        pass\n",
      "    \n",
      "    def when_on(self):\n",
      "        \"\"\"Return Series of bools\"\"\"\n",
      "    \n",
      "    def on_off_events(self):\n",
      "        # use self.metadata.minimum_[off|on]_duration\n",
      "        pass\n",
      "    \n",
      "    def discrete_appliance_activations(self):\n",
      "        \"\"\"\n",
      "        Return a Mask defining the start and end times of each appliance\n",
      "        activation.\n",
      "        \"\"\"\n",
      "    \n",
      "    def proportion_of_energy(self):\n",
      "        # get the old mask so we can put it back at the end\n",
      "        old_mask = self.get_loader_attribute('mask')\n",
      "        \n",
      "        # Mask out gaps from mains\n",
      "        self.set_loader_attributes(mask = self.mains.gaps())\n",
      "        proportion_of_energy = self.total_energy() / self. mains.total_energy()\n",
      "        self.set_loader_attributes(mask = old_mask)\n",
      "        return proportion_of_energy \n",
      "    \n",
      "    \n",
      "class ApplianceGroup(object):\n",
      "    \"\"\"\n",
      "    Implements many of the same methods as Appliance.\n",
      "    \n",
      "    Attributes\n",
      "    ----------\n",
      "    items : dict of Appliance or ApplianceGroup objects \n",
      "    \"\"\"\n",
      "    def __getattr__(self, key):\n",
      "        \"\"\"Gets a single appliance.\n",
      "        If no instance is specified then default to 1.\n",
      "        e.g. appliancegroup['toaster'] or ['toaster', 2]\n",
      "        \"\"\"\n",
      "        pass\n",
      "            \n",
      "    def select(self, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        e.g. \n",
      "        * select(category='lighting')\n",
      "        * select(category=['ict', 'lighting'])\n",
      "        * select([(fridge, 1), (tv, 1)]) # get specifically fridge 1 and tv 1\n",
      "        * select(name=['fridge', 'tv']) # get all fridges and tvs\n",
      "        * select(category='lighting', except={'room'=['kitchen lights']})\n",
      "        * select('all', except=[('tv', 1)])\n",
      "        \n",
      "        TODO: see if we can do select(category='lighting' | name='tree lights')\n",
      "        or select(energy > 100)??  Perhaps using:\n",
      "        * Python's eval function something like this:\n",
      "          >>> s = pd.Series(np.random.randn(5))\n",
      "          >>> eval('(x > 0) | (index > 2)', {'x':s, 'index':s.index})\n",
      "          Hmm, yes, maybe we should just implement this!  e.g.\n",
      "          select(\"(category == 'lighting') | (category == 'ict')\")\n",
      "          \n",
      "          But what about:\n",
      "          * select('total_energy > 100')\n",
      "          * select('mean(hours_on_per_day) > 3')\n",
      "          * select('max(hours_on_per_day) > 5')\n",
      "          * select('max(power) > 2000')\n",
      "          * select('energy_per_day > 2')\n",
      "          * select('rank_by_energy > 5') # top_k(5)\n",
      "          * select('rank_by_proportion > 0.2')\n",
      "          Maybe don't bother.  That's easy enough\n",
      "          to get with itemised_energy().  Although these are quite nice\n",
      "          and shouldn't be too hard.  Would need to only calculate\n",
      "          these stats if necessary though (e.g. by checking if 'total_energy'\n",
      "          is in the query string before running `eval`)\n",
      "          \n",
      "        * or numexpr: https://github.com/pydata/numexpr\n",
      "        * see Pandas.eval(): \n",
      "          * http://pandas.pydata.org/pandas-docs/stable/indexing.html#the-query-method-experimental\n",
      "          * https://github.com/pydata/pandas/blob/master/pandas/computation/eval.py#L119\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        A new ApplianceGroup\n",
      "        \"\"\"\n",
      "        \n",
      "    def groupby(self, **kwargs):\n",
      "        \"\"\"\n",
      "        e.g. groupby('category')\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        A new ApplianceGroup or ApplianceGroup objects\n",
      "        \"\"\"\n",
      "        \n",
      "    def total_on_duration(self):\n",
      "        \"\"\"Return timedelta\"\"\"\n",
      "        pass\n",
      "    \n",
      "    def on_durations(self):\n",
      "        self.get_unique_upstream_meters()\n",
      "        # for each meter, get the on time, \n",
      "        # assuming the on-power-threshold for the \n",
      "        # smallest appliance connected to that meter???\n",
      "        pass\n",
      "    \n",
      "    def activity_distribution(self, bin_size, timespan):\n",
      "        pass\n",
      "    \n",
      "    def when_on(self, on_power_threshold):\n",
      "        \"\"\"Return Series of bools\"\"\"\n",
      "        pass\n",
      "    \n",
      "    def cross_correlation(self):\n",
      "        \"\"\"Correlation between items.\"\"\"\n",
      "        pass\n",
      "        \n",
      "    def all_unique_meters(self):\n",
      "        \"\"\"Returns a set of all unique meters.  Some meters might measure the same appliances.\"\"\"\n",
      "        pass\n",
      "        \n",
      "    def unique_meters_without_double_counting(self):\n",
      "        \"\"\"Returns a set of all meters ensuring that each appliance only appears once.\"\"\"\n",
      "        # Gets unique meters from all appliances in this ApplianceGroup\n",
      "        # Creates graph of meters.  \n",
      "        # Removes all but the furthest upstream meters.\n",
      "        # Warns if we also measure energy for one or more appliances not in selection\n",
      "        pass\n",
      "        \n",
      "    def total_energy(self):\n",
      "        self.get_unique_upstream_meters()\n",
      "        # adds energy on a meter-by-meter basis\n",
      "        pass\n",
      "    \n",
      "    def on_off_events(self, minimum_state_duration):\n",
      "        pass\n",
      "    \n",
      "    def top_k(self, k=5):\n",
      "        \"\"\"Return new ApplianceGroup?\"\"\"\n",
      "        self.itemised_energy().ix[:k]\n",
      "    \n",
      "    def itemised_energy(self):\n",
      "        \"\"\" Needs to do it per-meter???  Return sorted.\n",
      "            'kitchen lights': 234.5\n",
      "            ['hall lights, bedroom lights'] : 32.1 # need to subtract kitchen lights energy from lighting circuit!\n",
      "        \"\"\" \n",
      "        # keys could be actual Appliance / ApplianceGroup objects?\n",
      "        # e.g. when we want to select top_k Appliances.\n",
      "        \n",
      "    def proportion_above(self, threshold_proportion):\n",
      "        \"\"\"Return new ApplianceGroup with all appliances whose proportion of energy usage is above threshold\"\"\"\n",
      "        \n",
      "    def itemised_proportions(self):\n",
      "        \"\"\"Proportion of energy per appliance. Return sorted.\"\"\"\n",
      "    \n",
      "    def power_series(self):\n",
      "        # Get all upstream meters. Add series.  Return generator of series.\n",
      "        # What happens if indicies don't match?  Automatically re-sample?  Or down-sample?\n",
      "        # Probably best to raise exception and make user pre-process???  How?\n",
      "        # lighting.resample('6S').power_series() ???? or\n",
      "        # lighting.preprocessing = [Resample('6S')]\n",
      "        # lighting.power_series()\n",
      "        \n",
      "    @property\n",
      "    def preprocessing(self, list_of_nodes):\n",
      "        for appliance in self.appliances:\n",
      "            appliance.preprocessing = list_of_nodes\n",
      "            \n",
      "    def init_new_dataset(self):\n",
      "        self.infer_and_set_meter_connections()\n",
      "        self.infer_and_set_dual_supply_appliances()\n",
      "            \n",
      "    def infer_and_set_meter_connections(self):\n",
      "        \"\"\"\n",
      "        Arguments\n",
      "        ---------\n",
      "        meters : list of Meter objects\n",
      "        \"\"\"\n",
      "        # Maybe this should be a stand-alone function which\n",
      "        # takes a list of meters???\n",
      "        \n",
      "    def infer_and_set_dual_supply_appliances(self):\n",
      "        pass\n",
      "    \n",
      "    def plot(self, how='stacked'):\n",
      "        \"\"\"\n",
      "        Arguments\n",
      "        ---------\n",
      "        stacked : {'stacked', 'heatmap', 'lines', 'snakey'}\n",
      "        \"\"\"\n",
      "        # pretty snakey:\n",
      "        # http://www.cl.cam.ac.uk/research/srg/netos/c-aware/joule/V4.00/\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The building layer\n",
      "\n",
      "* The meter hierarchy is structured into a tree / graph\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Electricity(ApplianceGroup):\n",
      "    \"\"\"\n",
      "    Attributes\n",
      "    ----------\n",
      "    mains : Mains object (which has references to relevant meters)\n",
      "\n",
      "    metadata : dict\n",
      "        nominal_voltage : float\n",
      "        \n",
      "    Assumptions\n",
      "    -----------\n",
      "    * exactly one Mains object per building\n",
      "    \"\"\"\n",
      "\n",
      "    def proportion_energy_submetered(self):\n",
      "        gaps = self.mains.gaps()\n",
      "        submetered_energy = 0.0\n",
      "        for meter in self.unique_downstream_meters():\n",
      "            meter.set_loader_attributes(mask = gaps)\n",
      "            submetered_energy += meter.total_energy().combined\n",
      "        return submetered_energy / self.mains.total_energy().combined\n",
      "        # TODO: save old mask so we can put it back?\n",
      "\n",
      "    def export(destination, key):\n",
      "        # Exports all meters and then the mains metadata and then \n",
      "        # exports each appliance in turn\n",
      "        \n",
      "    def import(store, key):\n",
      "        # Imports each meter \n",
      "        # then Mains object\n",
      "        # and then each appliance\n",
      "\n",
      "\n",
      "# EXAMPLES\n",
      "\n",
      "# get Mains energy\n",
      "electric.mains.energy().combined\n",
      "\n",
      "-----------\n",
      "\n",
      ">>> toaster = electric.appliances['toaster']\n",
      "Warning: 'toaster' is connected to a meter which measures multiple appliances\n",
      "\n",
      ">>> toaster\n",
      "Appliance(name='toaster', meters=[Meter(id=1, appliances=['toaster', 'sandwich maker'])])\n",
      "\n",
      ">>> toaster.on_duration()\n",
      "Warning: 'toaster' connected to a meter which measured multiple appliances\n",
      "timedelta(hours=123.0)\n",
      "\n",
      ">>> sm = electric.appliances['sandwich maker']\n",
      "Warning: 'sandwich maker' connected to a meter which measured multiple appliances\n",
      "\n",
      ">>> sm\n",
      "Appliance(name='sandwich maker', meters=[Meter(id=1, appliances=['toaster', 'sandwich maker'])])\n",
      "\n",
      ">>> sm.on_duration()\n",
      "Warning: 'sandwich maker' connected to a meter which measured multiple appliances\n",
      "timedelta(hours=123.0)\n",
      "\n",
      ">>> washer = electric.appliances['washing machine']\n",
      ">>> washer\n",
      "Appliance(name='washer', meters=[Meter(id=2, appliances=['washer']), Meter(id=3, appliances=['washer'])])\n",
      "\n",
      ">>> tv = electric.appliances['tv']\n",
      ">>> tv\n",
      "Appliance(name='tv', meters=[Meter(id=4)])\n",
      "\n",
      "-----\n",
      "\n",
      ">>> selection = electric.appliances.select(names=['sandwich maker', 'toaster', 'tv'])\n",
      "ApplianceGroup(Appliance(name='sandwich maker', meters=[Meter(id=1)]), \n",
      "               Appliance(name='toaster', meters=[Meter(id=1)]),\n",
      "               Appliance(name='tv', meters=[Meter(id=4)]))\n",
      "\n",
      "--------\n",
      "\n",
      ">>> lighting = electric.appliances.select(category='lighting')\n",
      "ApplianceGroup(Appliance(name='kitchen lights', meters=[Meter(id=5, upstream_meter=Meter(id=6))]),\n",
      "               Appliance(name='hall light', meters=[Meter(id=6, downstream_meters=[Meter(id=5)], appliances=['hall light'] etc)]))\n",
      "\n",
      ">>> lighting.total_energy()\n",
      "# Gets unique meters\n",
      "# Creates graph of meters.  Removes all but the furthest upstream meters.\n",
      "# Warns if we also measure energy for one or more appliances not in selection\n",
      "# adds energy on a meter-by-meter basis\n",
      "\n",
      ">>> lighting.on_durations()\n",
      "# gets unique upstream meters (as per energy()), \n",
      "# for each meter, get the on time, assuming the on-power-threshold for the smallest appliance connected to that meter???\n",
      "\n",
      ">>> lighting.itemised_energy()\n",
      "# Needs to do it per-meter???\n",
      "'kitchen lights': 234.5\n",
      "['hall lights, bedroom lights'] : 32.1 # need to subtract kitchen lights energy from lighting circuit!\n",
      "    \n",
      ">>> lighting.power_series()\n",
      "# Get all upstream meters. Add series.  Return generator of series.\n",
      "# What happens if indicies don't match?  Automatically re-sample?  Or down-sample?\n",
      "# Probably best to raise exception and make user pre-process???  How?\n",
      "# lighting.resample('6S').power_series() ???? or\n",
      "# lighting.insert_preprocessing_step(Resample('6S'))\n",
      "# lighting.power_series()\n",
      "\n",
      ">>> lighting.preprocessing = [Resample('6S')]\n",
      ">>> series = lighting.power_series()\n",
      "\n",
      "---------\n",
      "\n",
      ">>> groups = electric.appliances.groupby('category')\n",
      "ApplianceGroup(ApplianceGroup(category='lighting', [Appliance('kitchen lights'), Appliance('hall light')]),\n",
      "               ApplianceGroup(category='ICT', [Appliance('computer'), Appliance('laptop')]))\n",
      "\n",
      ">>> groups.itemised_energy()\n",
      "# get ApplianceGroup.total_energy() per appliance group\n",
      "'lighting': 234.1\n",
      "'ICT'     : 102.3\n",
      "    \n",
      "\n",
      "\n",
      "# iterate through appliances\n",
      "for appliance in electric.select('all'):\n",
      "    # do something with appliance\n",
      "    pass\n",
      "\n",
      "# get list of ApplianceGroups\n",
      "for appliance_group in electric.group_by('name'):\n",
      "    print(appliance_group.name, appliance_group.energy().combined)\n",
      "    \n",
      "electric.proportion_energy_submetered()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Building(object):   \n",
      "    \"\"\"\n",
      "    Attributes\n",
      "    ----------\n",
      "    \n",
      "    metadata : dict:\n",
      "        year_built: int, four digit year e.g. 2012\n",
      "        id: int\n",
      "        full_name: string, e.g. \"REDD/building1\".  Used to uniquely identify this building across all datasets.\n",
      "        \n",
      "    \"\"\"\n",
      "    def __init__(self):\n",
      "        self.metadata = {}\n",
      "        self.utility = Utility()\n",
      "    \n",
      "    def import(self, store, key):\n",
      "        self.metadata = store.load_metadata(key)\n",
      "        self.utility.import(store, key)\n",
      "                \n",
      "    def export(self, destination, key):\n",
      "        destination.write_metadata(key, self.metadata)\n",
      "        self.utility.export(destination, key + '/utility')\n",
      "        \n",
      "\n",
      "class Utility(object):\n",
      "    def __init__(self):\n",
      "        self.electric = Electricity()\n",
      "    \n",
      "    def import(self, store, key):\n",
      "        self.electric.import(store, key + '/electric')\n",
      "        \n",
      "    def export(self, destination, key):\n",
      "        # export metadata (if there is any?)\n",
      "        self.electric.export(store, key + '/electric')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The dataset layer\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I'd imagine the DataSet class working pretty much identically to the existing DataSet class"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class DataSet(object):\n",
      "    def __init__(self):\n",
      "        self.buildings = {}\n",
      "        \n",
      "    def import(self, store, key='/'):\n",
      "        self.metadata = store.load_metadata(key)\n",
      "        self._init_buildings(store, key)\n",
      "        return self\n",
      "        \n",
      "    def _init_buildings(self, store, key='/'):\n",
      "        buildings = self.store.building_numeric_ids(key)\n",
      "        for building_id in buildings:\n",
      "            building = Building(building_id)\n",
      "            building.import(store, key + 'building' + str(building_id))\n",
      "            self.buildings[building_id] = building\n",
      "\n",
      "    def export(self, destination, key='/'):\n",
      "        for id, building in self.buildings.iteritems():\n",
      "            building.export(destination, key + 'building' + str(id))\n",
      "            \n",
      "# EXAMPLE\n",
      "\n",
      "# Loading data\n",
      "\n",
      "# There are 'DataStore' objects, which know how to \n",
      "# read / write physical data stores (HDF5, CSV, Xively, REDD, HES, etc)\n",
      "\n",
      "# DataSet, Building, Electricity, Appliance and Meter objects\n",
      "# have an 'import' and an 'export' which handle\n",
      "# any DataStore subclass.\n",
      "\n",
      "raw_redd_directory = '/data/REDD/low_freq'\n",
      "reddstore = REDDStore(raw_redd_directory)\n",
      "redd = DataSet().import(reddstore)\n",
      "\n",
      "# at this point, all Buildings, Meters, Appliance (etc) objects\n",
      "# are constructed and their metadata loaded but no\n",
      "# power timeseries data is loaded into memory yet\n",
      "\n",
      "# Export to HDF5 (after applying any pre-processing that's enabled)\n",
      "destination = HDFDataStore('redd.h5')\n",
      "redd.export(destination)\n",
      "\n",
      "# Or export to NILMTK CSV\n",
      "destination = CSVDataStore('/data')\n",
      "redd.export(destination)\n",
      "\n",
      "# Or export to strict REDD\n",
      "destination = REDDStore('/data/redd_preprocessed')\n",
      "redd.export(destination)\n",
      "\n",
      "# get Appliance object for Fridge\n",
      "appliances = redd.buildings[1].utility.electric.\n",
      "fridge = appliances['fridge']\n",
      "# or could use ['fridge', 1'] or [('fridge', 1)] \n",
      "# - these are all equivalent!\n",
      "\n",
      "# Export just the fridge\n",
      "fridge.export(destination)\n",
      "\n",
      "# load power-data into memeory chunk by chunk:\n",
      "for series in fridge.power_series():\n",
      "    # do something with series.  It's just a\n",
      "    # pandas.Series.  The measurement is\n",
      "    # 'intelligently' selected given the mains measurements\n",
      "    # and a list of preferences.  Of course, these\n",
      "    # defaults can be overridded.\n",
      "\n",
      "# All the usual stats are enabled\n",
      "fridge.proportion_of_energy()\n",
      "fridge.on_durations()\n",
      "fridge.total_energy()\n",
      "fridge.total_on_duration()\n",
      "fridge.gaps()\n",
      "fridge.dropout_rate(ignore_gaps = True)\n",
      "\n",
      "# All stats functions check that\n",
      "# preconditions are met etc.\n",
      "\n",
      "# If we want any statistic to be calculated\n",
      "# for a specific time period then:\n",
      "# Control chunk periods or start and end dates:\n",
      "fridge.set_loader_attributes(period='D', start_date='2012-01-01')\n",
      "\n",
      "fridge.total_energy('per_period')\n",
      "'2012-01-01'  9.3\n",
      "'2012-01-02' 10.5\n",
      "'2012-01-03'  8.3\n",
      "...\n",
      "\n",
      "# or:\n",
      "fridge.set_loader_attributes(period='M')\n",
      "\n",
      "fridge.dropout_rate('per_period')\n",
      "'2012-01-01'  0.1\n",
      "'2012-02-01'  0.2\n",
      "'2012-03-01'  0.2\n",
      "...\n",
      "\n",
      "\n",
      "########################################################\n",
      "# We can also do some fun things with groups of appliances...\n",
      "#\n",
      "\n",
      "# And now we can group appliances by any metadata field.\n",
      "# For example, say we wanted to find the proportion of\n",
      "# energy per 'category' ('ICT', 'cold', 'hot' etc etc)\n",
      "by_category = appliances.groupby('category')\n",
      "by_category.itemised_proportions()\n",
      "'ICT': 0.34\n",
      "'cold': 0.04\n",
      "'heating': 0.39\n",
      "\n",
      "# Or say we wanted to train a NILM algorithm\n",
      "# as if each category were a single appliance:\n",
      "for category in by_category.items():\n",
      "    aggregate_power_for_category = category.power_series()\n",
      "\n",
      "# etc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Stuff that still needs thought\n",
      "\n",
      "* should we apply specific pre-processing to each dataset type (REDD, iAWE, etc)?  The current design says not; it says we should make sure all necessary pre-processing steps are done before each stats function.\n",
      "* Sketchy ideas for NILMResults:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NILMResults(ApplianceGroup):\n",
      "    \"\"\"\n",
      "    Attributes\n",
      "    ----------\n",
      "    nilm_metadata : dict\n",
      "        algorithm : dict:\n",
      "            class : string, e.g. \"FHMM\", \"CO\", etc\n",
      "            version : \n",
      "        trained_on : \n",
      "        disag_runtime : seconds\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "class Metrics(object):\n",
      "    \"\"\"\n",
      "    Attributes\n",
      "    ----------\n",
      "    disag_results : DisagResults\n",
      "    ground_truth : ApplianceGroup\n",
      "    \"\"\"\"\n",
      "\n",
      "    #------------ METRICS METHODS ----------------#\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}